{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up my environment for machine learning with some common libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Scikit-learn Libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, log_loss, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.feature_selection import RFE, mutual_info_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import DBSCAN\n",
    "# SciPy Libraries\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Statsmodels Library\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "### Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SPSS file\n",
    "data = pd.read_csv('Heart Failure Data.csv')  # Adjust the filename if necessary\n",
    "\n",
    "# Print the first few rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data =data.drop(columns=['StudyID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheak data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cheak missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Display the total number of missing values\n",
    "total_missing = data.isnull().sum().sum()\n",
    "print(f\"\\nTotal Missing Values in the Dataset: {total_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheaking duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate columns\n",
    "duplicate_columns = data.columns[data.columns.duplicated()].unique()\n",
    "\n",
    "# Display the duplicate columns (if any)\n",
    "if len(duplicate_columns) > 0:\n",
    "    print(f\"Duplicate columns found: {duplicate_columns}\")\n",
    "else:\n",
    "    print(\"No duplicate columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicate rowsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "\n",
    "# Display the duplicate rows (if any)\n",
    "if not duplicate_rows.empty:\n",
    "    print(\"Duplicate rows found:\")\n",
    "    print(duplicate_rows)\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliners decetion and Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Method is Best for Heart Disease Prediction?\n",
    "\n",
    "If the dataset is small and structured: Use IQR or Z-Score.\n",
    "\n",
    "If the dataset has high-dimensional features: Use Isolation Forest or LOF.\n",
    "\n",
    "If there are non-linear patterns: Use DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Oulters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting grid\n",
    "num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "fig, axes = plt.subplots(nrows=len(num_cols), ncols=2, figsize=(10, len(num_cols)*5))\n",
    "\n",
    "# Loop through numeric columns and plot histograms and scatter plots\n",
    "for i, col in enumerate(num_cols):\n",
    "    # Histogram\n",
    "    sns.histplot(data[col], kde=True, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f'Histogram of {col}')\n",
    "\n",
    "    # Detect outliers using IQR method\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "\n",
    "    # Scatter plot with outliers in a different color\n",
    "    sns.scatterplot(x=data.index, y=data[col], ax=axes[i, 1], color=['skyblue'])  # Normal points\n",
    "    sns.scatterplot(x=outliers.index, y=outliers[col], ax=axes[i, 1], color='red')  # Outliers in red\n",
    "    axes[i, 1].set_title(f'Scatter Plot of {col} with Outliers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove outliers by (Density-Based Spatial Clustering of Applications with Noise) DBSCAN because non-linear patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN-based outlier detection\n",
    "def detect_outliers_dbscan(data, eps=0.5, min_samples=5):\n",
    "    numerical_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Standardize data for DBSCAN\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "    # Identify outliers (DBSCAN labels outliers as -1)\n",
    "    outliers = pd.Series(labels == -1, index=numerical_data.index)\n",
    "    return outliers\n",
    "\n",
    "# DBSCAN-based outlier detection and replacing with median\n",
    "def impute_outliers_with_dbscan(data, eps=0.5, min_samples=5, max_iterations=5):\n",
    "    for iteration in range(max_iterations):\n",
    "        outliers = detect_outliers_dbscan(data, eps, min_samples)\n",
    "\n",
    "        # If no outliers are found, break\n",
    "        if outliers.sum() == 0:\n",
    "            print(f\"Outliers have been handled after {iteration + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        # Replace outliers with median\n",
    "        numerical_data = data.select_dtypes(include=[np.number])\n",
    "        for column in numerical_data.columns:\n",
    "            median_value = numerical_data[column].median()\n",
    "            data.loc[outliers, column] = median_value\n",
    "\n",
    "    return data\n",
    "\n",
    "# Check outliers in the original dataset\n",
    "outliers_before = detect_outliers_dbscan(data)\n",
    "print(\"Outliers detected before imputation:\", outliers_before.sum())\n",
    "\n",
    "# Apply the iterative DBSCAN-based imputation method\n",
    "data_imputed = impute_outliers_with_dbscan(data)\n",
    "\n",
    "# Check outliers after imputation\n",
    "outliers_after = detect_outliers_dbscan(data_imputed)\n",
    "print(\"\\nOutliers detected after iterative DBSCAN imputation:\", outliers_after.sum())\n",
    "\n",
    "# Verify if there are any outliers (should be 0)\n",
    "if outliers_after.sum() == 0:\n",
    "    print(\"\\nNo outliers detected after iterative imputation.\")\n",
    "else:\n",
    "    print(\"\\nThere are still outliers present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summery of Statistical Numberical Colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=[np.number]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summery of Statistical catagorical Colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=['category','object']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the terget Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['HF'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "HF_counts = data['HF'].value_counts()\n",
    "HF_percentage = HF_counts / HF_counts.sum() * 100\n",
    "\n",
    "# Pie Chart with Shadows (Simulated 3D Effect)\n",
    "plt.figure(figsize=(6, 4))\n",
    "colors = ['deepskyblue','lightcoral','red','olive'][:len(HF_counts)]\n",
    "\n",
    "plt.pie(\n",
    "    HF_counts,\n",
    "    labels=HF_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    explode=[0.1] * len(HF_counts),  # Slightly separate each slice\n",
    "    shadow=True  # Add shadow for depth\n",
    ")\n",
    "plt.title('Heart Failure (HF) Status')\n",
    "plt.axis('equal')  # Ensures the pie chart is circular\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Bar Chart with Enhancements\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "colors = sns.color_palette(\"cool\", len(HF_counts))  # Gradient color palette\n",
    "\n",
    "bars = ax.bar(HF_counts.index, HF_counts.values, color=['deepskyblue','lightcoral','red','olive'], edgecolor='white', linewidth=1.2)\n",
    "\n",
    "# Annotate bars\n",
    "for bar, percentage in zip(bars, HF_percentage):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.5,\n",
    "        f'{int(bar.get_height())} ({percentage:.1f}%)',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "ax.set_title('Heart Failure (HF)', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_xlabel('Heart Failure (HF)', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "sns.despine()  # Remove top and right spines for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Model Without Fearures selections(or With 47 Columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detailed encoding plan with specific numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define Encoding Methods\n",
    "ordinal_features = ['NYHA', 'LVEF', 'MR', 'EuroScore', 'GraceScore']\n",
    "onehot_features = ['Sex', 'Smoking', 'HTN', 'DM', 'FH', 'DL', 'BA', 'CXR', 'ECG', 'RWMA',\n",
    "                   'MI', 'ACS', 'Thrombolysis','Chest_pain', 'TC', 'LDL', 'HDL', 'TG', 'BNP']\n",
    "frequency_features = ['Wall']\n",
    "\n",
    "# Assuming 'ndata' is your DataFrame and ordinal_mapping is already defined\n",
    "\n",
    "# Apply Ordinal Encoding\n",
    "for col in ordinal_features:\n",
    "    if col in ndata.columns:\n",
    "        ndata[col] = ndata[col].map(ordinal_mapping[col])  # Apply mapping to convert categorical to numeric\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "ndata = pd.get_dummies(ndata, columns=[col for col in onehot_features if col in ndata.columns], drop_first=True)\n",
    "\n",
    "# Apply Frequency Encoding for High-Cardinality Variable\n",
    "for col in frequency_features:\n",
    "    if col in ndata.columns:\n",
    "        freq_encoding = ndata[col].value_counts(normalize=True)  # Compute frequency\n",
    "        ndata[col] = ndata[col].map(freq_encoding)  # Replace category with frequency\n",
    "\n",
    "# Display first 15 rows for inspection\n",
    "print(ndata.head(15).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Identify numerical columns (float64 type columns)\n",
    "numerical_columns = ndata.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling only to the numerical columns\n",
    "ndata_scaled = ndata.copy()  # Create a copy of the original dataframe\n",
    "ndata_scaled[numerical_columns] = scaler.fit_transform(ndata[numerical_columns])\n",
    "\n",
    "# Display the first few rows of the scaled DataFrame\n",
    "print(ndata_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DHF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
